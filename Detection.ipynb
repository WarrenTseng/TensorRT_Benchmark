{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd9a5bb8",
   "metadata": {},
   "source": [
    "## Create a model and export it as ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a1b549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "batch_size = 32\n",
    "device = 'cuda'\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s').to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b12971",
   "metadata": {},
   "source": [
    "After training..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429b149a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './models/obj.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf201976",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.randn(batch_size, 3, 256, 256).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(model(dummy_input).shape)\n",
    "\n",
    "torch.onnx.export(model, dummy_input, './models/obj.onnx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6b7e2f",
   "metadata": {},
   "source": [
    "## Convert the ONNX model to TensorRT engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79cd1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart notebook kernel\n",
    "import os\n",
    "os._exit(00)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebfc9d3",
   "metadata": {},
   "source": [
    "- Convert to FP32 engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fe611a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!trtexec --onnx=./models/obj.onnx --saveEngine=./models/obj_32.engine "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313a0f20",
   "metadata": {},
   "source": [
    "- Convert to FP16 engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbc34ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!trtexec --onnx=./models/obj.onnx --saveEngine=./models/obj_16.engine --fp16 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e495a3cf",
   "metadata": {},
   "source": [
    "## Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c119e5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark import NativeTorchBenchmark, TensorRTBehcnmark\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "n_infers = 100\n",
    "batch_size = 32\n",
    "input_image = np.random.normal(size=[batch_size, 3, 256, 256])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d885f42",
   "metadata": {},
   "source": [
    "- Native PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "568b1654",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ 2023-5-2 Python-3.8.10 torch-2.1.0a0+fe05266 CUDA:0 (NVIDIA GeForce RTX 3060, 12042MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m /root/.cache/torch/hub/requirements.txt not found, check failed.\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
    "\n",
    "pt_bm = NativeTorchBenchmark(n_infers=n_infers,\n",
    "                             batch_size=batch_size,\n",
    "                             samples=input_image,\n",
    "                             model_arch=model,\n",
    "                             model_ckpt='./models/obj.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "faae2d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:04<00:00, 21.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Throughputs (Native PyTorch): 673.2782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pt_bm.benchmark()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e052c06e",
   "metadata": {},
   "source": [
    "- TensorRT FP32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81cfd410",
   "metadata": {},
   "outputs": [],
   "source": [
    "trt_bm_fp32 = TensorRTBehcnmark(n_infers=n_infers,\n",
    "                                batch_size=batch_size,\n",
    "                                samples=input_image,\n",
    "                                engine_path='./models/obj_32.engine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ae68d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:02<00:00, 36.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Throughputs: 1158.5222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trt_bm_fp32.benchmark()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0269766f",
   "metadata": {},
   "source": [
    "- TensorRT FP16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e124f9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trt_bm_fp16 = TensorRTBehcnmark(n_infers=n_infers,\n",
    "                                batch_size=batch_size,\n",
    "                                samples=input_image,\n",
    "                                engine_path='./models/obj_16.engine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "996e02aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:01<00:00, 53.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Throughputs: 1722.2369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trt_bm_fp16.benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925cfca8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
